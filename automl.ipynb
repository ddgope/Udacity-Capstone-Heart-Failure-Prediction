{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "## Dependencies \n",
    "\n",
    "All the dependencies needed to complete the project appear here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-7c77293c9f1c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-7c77293c9f1c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install XGBoost\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install XGBoost\n",
    "\n",
    "anaconda/envs/azureml_py36/lib/libxgboost.so: undefined symbol: XGBoosterUnserializeFromBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612382549902
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import joblib\n",
    "\n",
    "from azureml.core.environment import Environment \n",
    "from azureml.core.model import InferenceConfig \n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace\n",
    "\n",
    "The `config.json` file is downloaded from Azure environment and has to be in the project folder in order for this cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612382555927
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment\n",
    "I am creating an experiment named `heart-failure-prediction` and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure.\n",
    "\n",
    "The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612382568571
    }
   },
   "outputs": [],
   "source": [
    "# Choose a name for the run history container in the workspace.\n",
    "\n",
    "experiment_name = 'heart-failure-prediction'\n",
    "project_folder = './capstone-project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "experiment\n",
    "\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach a cluster\n",
    "\n",
    "We will need to create a [compute target](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#compute-target) for the AutoML run. In case the compute target (named `compute-cluster` in this script) is not found, a new one is created using the default AmlCompute as the training compute resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612382581963
    }
   },
   "outputs": [],
   "source": [
    "# max_nodes should be no greater than 4.\n",
    "\n",
    "# Choose a name for the cluster\n",
    "cpu_cluster_name = \"compute-cluster2\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute cluster...')\n",
    "    # Poll for a minimum number of nodes (min_nodes = 1). \n",
    "    # If no min node count is provided it uses the scale settings for the cluster.\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS3_v2', min_nodes=1, max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "The dataset used is taken from [Kaggle](https://www.kaggle.com/andrewmvd/heart-failure-clinical-data) and the data comes from 299 patients with heart failure collected at the Faisalabad Institute of Cardiology and at the Allied Hospital in Faisalabad (Punjab, Pakistan), during Aprilâ€“December 2015. The patients consisted of both women (105) and men (194), and the main task of the project is to classify the patients based on their odds of survival.\n",
    "\n",
    "Dataset features:\n",
    "\n",
    "| Feature | Explanation |\n",
    "| :---: | :---: |\n",
    "| *age* | Age of patient |\n",
    "| *anaemia* | Decrease of red blood cells or hemoglobin |\n",
    "| *creatinine-phosphokinase* | Level of the CPK enzyme in the blood |\n",
    "| *diabetes* | Whether the patient has diabetes or not |\n",
    "| *ejection_fraction* | Percentage of blood leaving the heart at each contraction |\n",
    "| *high_blood_pressure* | Whether the patient has hypertension or not |\n",
    "| *platelets* | Platelets in the blood |\n",
    "| *serum_creatinine* | Level of creatinine in the blood |\n",
    "| *serum_sodium* | Level of sodium in the blood |\n",
    "| *sex* | Female (F) or Male (M) |\n",
    "| *smoking* | Whether the patient smokes or not |\n",
    "| *time* | Follow-up period |\n",
    "| *DEATH_EVENT* | Whether the patient died during the follow-up period |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612382598440
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./heart_failure_clinical_records_dataset.csv')\n",
    "\n",
    "found = False\n",
    "key = \"heart-failure-prediction\"\n",
    "description_text = \"Prediction of survival of patients with heart failure\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        dataset = ws.datasets[key] \n",
    "\n",
    "if not found:\n",
    "        # Create AML Dataset and register it into Workspace\n",
    "        my_dataset = 'https://raw.githubusercontent.com/ddgope/Udacity-Capstone-Heart-Failure-Prediction/master/heart_failure_clinical_records_dataset.csv'\n",
    "        dataset = Dataset.Tabular.from_delimited_files(my_dataset)        \n",
    "        # Register Dataset in Workspace\n",
    "        dataset = dataset.register(workspace=ws,\n",
    "                                   name=key,\n",
    "                                   description=description_text)\n",
    "                                \n",
    "# Preview of the first five rows\n",
    "print(data.head())\n",
    "\n",
    "# Explore data\n",
    "print(data.describe())\n",
    "\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.describe()\n",
    "\n",
    "# Data columns\n",
    "df.columns = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'ejection_fraction', 'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time', 'DEATH_EVENT']\n",
    "x = df[['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'ejection_fraction', 'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']]\n",
    "y = df[['DEATH_EVENT']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "Here is an overview of the `automl` settings and configuration I used for the AutoML run:\n",
    "\n",
    "`\"n_cross_validations\": 2`\n",
    "\n",
    "This parameter sets how many cross validations to perform, based on the same number of folds (number of subsets). As one cross-validation could result in overfit, in my code I chose 2 folds for cross-validation; thus the metrics are calculated with the average of the 2 validation metrics.\n",
    "\n",
    "`\"primary_metric\": 'accuracy'`\n",
    "\n",
    "I chose accuracy as the primary metric as it is the default metric used for classification tasks.\n",
    "\n",
    "`\"enable_early_stopping\": True`\n",
    "\n",
    "It defines to enable early termination if the score is not improving in the short term. In this experiment, it could also be omitted because the _experiment_timeout_minutes_ is already defined below.\n",
    "\n",
    "`\"max_concurrent_iterations\": 4`\n",
    "\n",
    "It represents the maximum number of iterations that would be executed in parallel.\n",
    "\n",
    "`\"experiment_timeout_minutes\": 20`\n",
    "\n",
    "This is an exit criterion and is used to define how long, in minutes, the experiment should continue to run. To help avoid experiment time out failures, I used the value of 20 minutes.\n",
    "\n",
    "`\"verbosity\": logging.INFO`\n",
    "\n",
    "The verbosity level for writing to the log file.\n",
    "\n",
    "`compute_target = compute_target`\n",
    "\n",
    "The Azure Machine Learning compute target to run the Automated Machine Learning experiment on.\n",
    "\n",
    "`task = 'classification'`\n",
    "\n",
    "This defines the experiment type which in this case is classification. Other options are _regression_ and _forecasting_.\n",
    "\n",
    "`training_data = dataset`\n",
    "\n",
    "The training data to be used within the experiment. It should contain both training features and a label column - see next parameter.\n",
    "\n",
    "`label_column_name = 'DEATH_EVENT'` \n",
    "\n",
    "The name of the label column i.e. the target column based on which the prediction is done.\n",
    "\n",
    "`path = project_folder`\n",
    "\n",
    "The full path to the Azure Machine Learning project folder.\n",
    "\n",
    "`featurization = 'auto'`\n",
    "\n",
    "This parameter defines whether featurization step should be done automatically as in this case (_auto_) or not (_off_).\n",
    "\n",
    "`debug_log = 'automl_errors.log`\n",
    "\n",
    "The log file to write debug information to.\n",
    "\n",
    "`enable_onnx_compatible_models = False`\n",
    "\n",
    "I chose not to enable enforcing the ONNX-compatible models at this stage. However, I will try it in the future. For more info on Open Neural Network Exchange (ONNX), please see [here](https://docs.microsoft.com/en-us/azure/machine-learning/concept-onnx).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612382615679
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Automl settings\n",
    "\n",
    "automl_settings = {\"n_cross_validations\": 2,\n",
    "                    \"primary_metric\": 'accuracy',\n",
    "                    \"enable_early_stopping\": True,\n",
    "                    \"max_concurrent_iterations\": 4,\n",
    "                    \"experiment_timeout_minutes\": 25,\n",
    "                    \"verbosity\": logging.INFO                   \n",
    "                    }\n",
    "\n",
    "# Parameters for AutoMLConfig\n",
    "\n",
    "automl_config = AutoMLConfig(compute_target = compute_target,\n",
    "                            task='classification',\n",
    "                            training_data=dataset,\n",
    "                            label_column_name='DEATH_EVENT',\n",
    "                            path = project_folder,\n",
    "                            featurization= 'auto',\n",
    "                            debug_log = \"automl_errors.log\",\n",
    "                            enable_onnx_compatible_models=False,\n",
    "                            blocked_models='XGBoostClassifier', #XGBoostClassifier = 'XGBoostClassifier'\n",
    "                            **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612383725503
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote.\n",
      "Running on remote compute: compute-cluster2\n",
      "Parent Run ID: AutoML_afc8f16a-8a6b-41e9-ac0d-958c70470a91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Submit the experiment\n",
    "remote_run = experiment.submit(automl_config, show_output = True)\n",
    "remote_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612383975181
    }
   },
   "outputs": [],
   "source": [
    "# get_status()\n",
    "# Fetch the latest status of the run. It should show 'Completed'\n",
    "print(\"Run Status: \",remote_run.get_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "In the cell below, I use the `RunDetails` widget and show the children runs of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612383984076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "RunDetails(remote_run).show()\n",
    "\n",
    "# Get details from each run\n",
    "for child_run in remote_run.get_children():\n",
    "    print('===================================================')\n",
    "    print(child_run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "In the cell below, I get the best model from the automl experiment and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612384051283
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "best_run, fitted_model = remote_run.get_output()\n",
    "\n",
    "# get_metrics()\n",
    "# Returns the metrics\n",
    "print(\"Best run metrics :\",best_run.get_metrics())\n",
    "print('===================================================')\n",
    "\n",
    "# get_details()\n",
    "# Returns a dictionary with the details for the run\n",
    "print(\"Best run details :\",best_run.get_details())\n",
    "print('===================================================')\n",
    "\n",
    "# get_properties()\n",
    "# Fetch the latest properties of the run from the service\n",
    "print(\"Best run properties :\",best_run.get_properties())\n",
    "print('===================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = remote_run.get_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612384084492
    }
   },
   "outputs": [],
   "source": [
    "best_run.get_file_names()\n",
    "\n",
    "# Download the yaml file that includes the environment dependencies\n",
    "best_run.download_file('outputs/conda_env_v_1_0_0.yml', 'env.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Download the model file\n",
    "\n",
    "best_run.download_file('outputs/model.pkl', 'heart_disease_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612384096483
    }
   },
   "outputs": [],
   "source": [
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612384106353
    }
   },
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612384120796
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_run.register_model(model_name = \"heart_disease_model.pkl\", model_path = './outputs/')\n",
    "\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Based on Another Metric\n",
    "\n",
    "Show the run and model that has the highest **AUC_weighted** and the one with the smallest **average_precision_score_weighted** value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612384358504
    }
   },
   "outputs": [],
   "source": [
    "lookup_metric = \"AUC_weighted\"\n",
    "best_run, fitted_model = remote_run.get_output(metric = lookup_metric)\n",
    "print('========================================================')\n",
    "print(\"Based on AUC_weighted: \",best_run)\n",
    "print(fitted_model)\n",
    "\n",
    "lookup_metric = \"average_precision_score_weighted\"\n",
    "best_run, fitted_model = remote_run.get_output(metric = lookup_metric)\n",
    "print('========================================================')\n",
    "print(\"Based on average_precision_score_weighted: \",best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model\n",
    "\n",
    "As the best model coming from AutoML run has better accuracy than the one coming from the HyperDrive run, I deploy it in the cell below, register it, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612384627131
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model = remote_run.register_model(model_name = 'heart_disease_model.pkl')\n",
    "print(remote_run.model_id)\n",
    "\n",
    "# https://knowledge.udacity.com/questions/463620\n",
    "\n",
    "environment = best_run.get_environment()\n",
    "entry_script='inference/scoring.py'\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', entry_script)\n",
    "\n",
    "\n",
    "inference_config = InferenceConfig(entry_script = entry_script, environment = environment)\n",
    "\n",
    "# Deploying the model via ACI WebService\n",
    "# https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/machine-learning/how-to-deploy-azure-container-instance.md\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                                    memory_gb = 1, \n",
    "                                                    auth_enabled= True, \n",
    "                                                    enable_app_insights= True)\n",
    "\n",
    "service = Model.deploy(ws, \"aciservice\", [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment(show_output = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612384813062
    }
   },
   "outputs": [],
   "source": [
    "# Getting the service state\n",
    "# The scorig URI & the primary authentication key are copied to the endpoint.py file in order to test the deployed service.\n",
    "# The Swagger URI can be used in Swagger UI: https://petstore.swagger.io/ For more info, please see the relevant part in the README file.\n",
    "\n",
    "# Authentication is enabled, so I use the get_keys method to retrieve the primary and secondary authentication keys:\n",
    "primary, secondary = service.get_keys()\n",
    "\n",
    "print('Service state: ' + service.state)\n",
    "print('Service scoring URI: ' + service.scoring_uri)\n",
    "print('Service Swagger URI: ' + service.swagger_uri)\n",
    "print('Service primary authentication key: ' + primary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consume the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612384880056
    }
   },
   "outputs": [],
   "source": [
    "# Sending a request to the deployed web service to test it: consuming model endpoint\n",
    "%run endpoint.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the uri's in variables:\n",
    "scoring_uri = 'http://64f66fd9-4942-4998-b945-a726512c767a.southcentralus.azurecontainer.io/score'\n",
    "\n",
    "key = 'q0KNVV8uahLC1jGacIbteTFeiAXn1lmf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# URL for the web service, should be similar to:\n",
    "# 'http://8530a665-66f3-49c8-a953-b82a2d312917.eastus.azurecontainer.io/score'\n",
    "scoring_uri = scoring_uri\n",
    "# If the service is authenticated, set the key or token\n",
    "key = key\n",
    "\n",
    "# Two sets of data to score, so we get two results back\n",
    "data = {\"data\":\n",
    "        [\n",
    "          {\n",
    "           \"age\": 75, \n",
    "           \"anaemia\": 0, \n",
    "           \"creatinine_phosphokinase\": 582, \n",
    "           \"diabetes\": 0, \n",
    "           \"ejection_fraction\": 20, \n",
    "           \"high_blood_pressure\": 1, \n",
    "           \"platelets\": 265000, \n",
    "           \"serum_creatinine\": 1.9, \n",
    "           \"serum_sodium\": 130, \n",
    "           \"sex\": 1, \n",
    "           \"smoking\": 0,\n",
    "           \"time\": 4\n",
    "          },\n",
    "          {\n",
    "           \"age\": 30, \n",
    "           \"anaemia\": 0, \n",
    "           \"creatinine_phosphokinase\": 2656, \n",
    "           \"diabetes\": 1, \n",
    "           \"ejection_fraction\": 30, \n",
    "           \"high_blood_pressure\": 0, \n",
    "           \"platelets\": 305000, \n",
    "           \"serum_creatinine\": 2.3, \n",
    "           \"serum_sodium\": 130, \n",
    "           \"sex\": 1, \n",
    "           \"smoking\": 0,\n",
    "           \"time\": 30\n",
    "          },\n",
    "      ]\n",
    "    }\n",
    "# Convert to JSON string\n",
    "input_data = json.dumps(data)\n",
    "with open(\"data.json\", \"w\") as _f:\n",
    "    _f.write(input_data)\n",
    "\n",
    "# Set the content type\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "# If authentication is enabled, set the authorization header\n",
    "headers['Authorization'] = f'Bearer {key}'\n",
    "\n",
    "# Make the request and display the response\n",
    "resp = requests.post(scoring_uri, input_data, headers=headers)\n",
    "print(resp.json())\n",
    "print(\"++++++++++++++++++++++++++++++\")\n",
    "print(\"Expected result: [true, true], where 'true' means '1' as result in the 'DEATH_EVENT' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1612385060829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Printing the logs\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the service\n",
    "Putting the deletion of the service in a separate cell to avoid accidentally running the cell before finishing the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Service.delete()\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
